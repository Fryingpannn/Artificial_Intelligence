drugs discussion:
-----------------

The naive-bayes model's accuracy and F1 score do not change as there is no randomness and the probabilities are always the same through each iteration with the same data.

The base decision tree's accuracy and F1 score doesn't changes as the decision tree is trained in a non-deterministic way so there's no randomness.

The top decision tree's accuracy and F1 score changes due to the same reason as the base decision tree. At each step, the learners consider all possible features that have not been used to split the data, and finds the splits that maximizes the information gain.

The perceptron's model has no changes because the default value for the random state parameter is an integer and thus the results are reproducible across multiple function calls.

The base multi-layered perceptron has different results across iterations because it is using the stochastic gradient descent as the solver. The weight initializations are always randomnized and thus there are differents in ouputs.

The top multi-layered perceptron also has different results for the same reason as the base one but also includes the grid search with multiple parameters changes for the activation function, hidden layer sizes and solver. Thus, different combination of parameters would result in differences in accuracies and F1 scores.

Of course, it should be noted that the data never changes across all iterations for each model. This plays a big part as to why the differences are small even when present.